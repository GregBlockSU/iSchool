{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3279703b-bf9e-4441-ad4a-ffc37cf9ae28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cassandra\n"
     ]
    }
   ],
   "source": [
    "# 1. In Spark, set up a Spark session that is ready to talk with Cassandra.\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# CASSANDRA CONFIGURATION\n",
    "cassandra_host = \"cassandra\"\n",
    "\n",
    "# Spark init\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName('jupyter-pyspark') \\\n",
    "      .config(\"spark.cassandra.connection.host\", cassandra_host) \\\n",
    "      .config(\"spark.jars.packages\",\"com.datastax.spark:spark-cassandra-connector-assembly_2.12:3.1.0\")\\\n",
    "    .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "print(cassandra_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "93c5db88-9071-44af-9fd3-fa17d4a9387a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- 2020census: long (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- description: string (nullable = true)\n",
      " |-- dew_point: double (nullable = true)\n",
      " |-- latitude: double (nullable = true)\n",
      " |-- longitude: double (nullable = true)\n",
      " |-- moon_phase: double (nullable = true)\n",
      " |-- pct_clouds: long (nullable = true)\n",
      " |-- pct_humidity: long (nullable = true)\n",
      " |-- pressure: long (nullable = true)\n",
      " |-- rainfall: double (nullable = true)\n",
      " |-- snowfall: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- temperature.day: double (nullable = true)\n",
      " |-- temperature.eve: double (nullable = true)\n",
      " |-- temperature.max: double (nullable = true)\n",
      " |-- temperature.min: double (nullable = true)\n",
      " |-- temperature.morn: double (nullable = true)\n",
      " |-- temperature.night: double (nullable = true)\n",
      " |-- timezone: string (nullable = true)\n",
      " |-- uv_index: double (nullable = true)\n",
      " |-- wind.direction_deg: long (nullable = true)\n",
      " |-- wind.gust: double (nullable = true)\n",
      " |-- wind.speed: double (nullable = true)\n",
      "\n",
      "There are 1600 records\n"
     ]
    }
   ],
   "source": [
    "# 3. To deal with the amount of data associated with the weather.com data set, \n",
    "# you decide to start with a smaller sample data set. The dataset contains 7 \n",
    "# days of weather information for major U.S. cities, with one row being weather \n",
    "# information for a single city on a single day. Load the data set located \n",
    "# at /home/jovyan/datasets/weather/weather.json and use printSchema() \n",
    "# to inspect the schema.\n",
    "file_path = 'file:///home/jovyan/datasets/weather/weather.json'\n",
    "weather = spark.read.json(file_path)\n",
    "weather.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "27b58250-8c5e-416f-adc1-a7a5bdd33b6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1600 records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 8 unique date records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 30:===============================================>      (177 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 200 unique city + state records\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# 4. Look at rows of data in the sample data set. Profile the data to \n",
    "# determine what should be used as the partition and cluster key:\n",
    "# a. First: Find the minimal candidate key. Which columns serve as a key for each row? \n",
    "# NOTE: You canNOT use 2020census as that is a population figure and \n",
    "# coincidentally unique.\n",
    "# b. Next: Prove your key works. In Spark:\n",
    "#  i. Get a count of rows in the entire DataFrame.\n",
    "#  ii. Get a count rows when you select your key columns and use distinct() to \n",
    "#      remove duplicates.\n",
    "#  iii. If the row counts are the name, that’s a candidate key. Include the \n",
    "#       code and output in the screenshot.\n",
    "# c. A Cassandra row key consists of a partition and cluster key. For \n",
    "# this example, use the column that will guarantee to be storing data \n",
    "# in increasing order over time (append only) as your cluster key. \n",
    "# The other column (or columns) should be the partition\n",
    "\n",
    "# weather.show()\n",
    "\n",
    "print(f\"There are {weather.count()} records\")\n",
    "print(f\"There are {weather.select('date').distinct().count()} unique date records\")\n",
    "print(f\"There are {weather.select('city', 'state').distinct().count()} unique city + state records\")\n",
    "\n",
    "# candidate keys:\n",
    "# partition key: state, city\n",
    "# cluster key: date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1debad9a-db41-4c9d-9dd8-e72ac37fa8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. With your keys figured out, it’s time to create your table. Using\n",
    "# the CQL Shell, write an CQL query to create a table called \n",
    "# daily_city_weather. Include all columns in the source data set, \n",
    "# and make sure to set your partition and cluster keys, as designed. \n",
    "# Show the CQL query and the output in the screenshot. Include an \n",
    "# additional screenshot of the describe command on this table. \n",
    "# ADVICE: Write your create table in a text editor then paste it into CQL, \n",
    "# as the command line can be a tad unforgiving.\n",
    "\n",
    "# note, we are going to use spark to forward our queries to Cassandra\n",
    "!pip install -q cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "df8d0891-b4fd-4917-9edb-d6b2607bb10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE glab.weather\n",
      "(\n",
      "    census2020 int,\n",
      "    city  text,\n",
      "    condition  text,\n",
      "    weatherdate  date,\n",
      "    description  text,\n",
      "    dew_point decimal,\n",
      "    latitude decimal,\n",
      "    longitude decimal,\n",
      "    moon_phase decimal,\n",
      "    pct_clouds int,\n",
      "    pct_humidity int,\n",
      "    pressure int,\n",
      "    rainfall decimal,\n",
      "    snowfall decimal,\n",
      "    state text,\n",
      "    temperature_day decimal,\n",
      "    temperature_eve decimal,\n",
      "    temperature_max decimal,\n",
      "    temperature_min decimal,\n",
      "    temperature_morn decimal,\n",
      "    temperature_night decimal,\n",
      "    timezone text,\n",
      "    uv_index decimal,\n",
      "    wind_direction_deg int,\n",
      "    wind_gust decimal,\n",
      "    wind_speed decimal,\n",
      "    PRIMARY KEY ( (state, city), weatherdate)\n",
      ");\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#5 continued\n",
    "create_table_sql = '''\n",
    "CREATE TABLE glab.weather\n",
    "(\n",
    "    census2020 int,\n",
    "    city  text,\n",
    "    condition  text,\n",
    "    weatherdate  date,\n",
    "    description  text,\n",
    "    dew_point decimal,\n",
    "    latitude decimal,\n",
    "    longitude decimal,\n",
    "    moon_phase decimal,\n",
    "    pct_clouds int,\n",
    "    pct_humidity int,\n",
    "    pressure int,\n",
    "    rainfall decimal,\n",
    "    snowfall decimal,\n",
    "    state text,\n",
    "    temperature_day decimal,\n",
    "    temperature_eve decimal,\n",
    "    temperature_max decimal,\n",
    "    temperature_min decimal,\n",
    "    temperature_morn decimal,\n",
    "    temperature_night decimal,\n",
    "    timezone text,\n",
    "    uv_index decimal,\n",
    "    wind_direction_deg int,\n",
    "    wind_gust decimal,\n",
    "    wind_speed decimal,\n",
    "    PRIMARY KEY ( (state, city), weatherdate)\n",
    ");\n",
    "\n",
    "'''\n",
    "\n",
    "from cassandra.cluster import Cluster\n",
    "with Cluster([cassandra_host]) as cluster:\n",
    "    session = cluster.connect()\n",
    "    session.execute(create_table_sql)\n",
    "print(create_table_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "15fd39a9-111e-4100-bbb2-22606ecdba1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Write Spark code to save the JSON DataFrame into your Cassandra table. \n",
    "# Make sure the column names are the same. Read the data back out and \n",
    "# make sure you have the same number of rows in the\n",
    "# DataFrame and in the Cassandra table. This will be further proof that \n",
    "# your Cassandra row key is set up correctly. Provide Spark code to \n",
    "# save the data to Cassandra and then a screenshot of a select \n",
    "# statement and output in the CQL shell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b8b4137e-add8-4dd3-b503-d283d1884388",
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_2 = weather.toDF(\"census2020\",\n",
    "    \"city\",\n",
    "    \"condition\",\n",
    "    \"weatherdate\",\n",
    "    \"description\",\n",
    "    \"dew_point\",\n",
    "    \"latitude\",\n",
    "    \"longitude\",\n",
    "    \"moon_phase\",\n",
    "    \"pct_clouds\",\n",
    "    \"pct_humidity\",\n",
    "    \"pressure\",\n",
    "    \"rainfall\",\n",
    "    \"snowfall\",\n",
    "    \"state\",\n",
    "    \"temperature_day\",\n",
    "    \"temperature_eve\",\n",
    "    \"temperature_max\",\n",
    "    \"temperature_min\",\n",
    "    \"temperature_morn\",\n",
    "    \"temperature_night\",\n",
    "    \"timezone\",\n",
    "    \"uv_index\",\n",
    "    \"wind_direction_deg\",\n",
    "    \"wind_gust\",\n",
    "    \"wind_speed\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a863044c-19a4-4c07-acdd-10fe6142a297",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#5 continued\n",
    "weather_2.write.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .mode(\"Append\")\\\n",
    "    .option(\"table\", \"weather\")\\\n",
    "    .option(\"keyspace\", \"glab\")\\\n",
    "    .save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a1712c98-ac0d-41b3-9bf2-2dc7a808e542",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather_2 has 1600 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "weather_2 = spark.read.format(\"org.apache.spark.sql.cassandra\")\\\n",
    "    .option(\"table\", \"weather\")\\\n",
    "    .option(\"keyspace\", \"glab\")\\\n",
    "    .load()\n",
    "print(f\"weather_2 has {weather_2.count()} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4f46b35d-4ace-4aed-a00b-98af77c765cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [city#989, state#988, condition#992, description#993, temperature_day#1003]\n",
      "+- BatchScan[state#988, city#989, condition#992, description#993, temperature_day#1003] Cassandra Scan: glab.weather\n",
      " - Cassandra Filters: [[\"state\" = ?, New York],[\"city\" = ?, Syracuse]]\n",
      " - Requested Columns: [state,city,condition,description,temperature_day]\n",
      "\n",
      "\n",
      "The query is \n",
      "SELECT city, state, condition, description, temperature_day \n",
      "FROM daily_city_weather \n",
      "WHERE city = 'Syracuse' AND state = 'New York';\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 8. Write the same query as Question 7 but using Spark SQL. Register the \n",
    "# data from Cassandra as the Temp View daily_city_weather, then use Spark \n",
    "# SQL To filter on “Syracuse, NY.” Instead of showing the output, \n",
    "# explain() the Spark query to prove the filter is being passed through \n",
    "# to Cassandra. (The filter should NOT be happening in Spark—Welcome \n",
    "# to big data country!)\n",
    "weather_2.createOrReplaceTempView(\"daily_city_weather\")\n",
    "query = '''\n",
    "SELECT city, state, condition, description, temperature_day \n",
    "FROM daily_city_weather \n",
    "WHERE city = 'Syracuse' AND state = 'New York';\n",
    "'''\n",
    "spark.sql(query).explain()\n",
    "print(f\"The query is {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3c15cb19-6d87-4564-8999-d879c4dd7963",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(1) Project [city#989, state#988, condition#992, description#993, temperature_day#1003]\n",
      "+- *(1) Filter (condition#992 = Rain)\n",
      "   +- BatchScan[state#988, city#989, condition#992, description#993, temperature_day#1003] Cassandra Scan: glab.weather\n",
      " - Cassandra Filters: [[\"weatherdate\" = ?, 2021-10-22]]\n",
      " - Requested Columns: [state,city,condition,description,temperature_day]\n",
      "\n",
      "\n",
      "The query is \n",
      "SELECT city, state, condition, description, temperature_day \n",
      "FROM daily_city_weather \n",
      "WHERE condition = 'Rain' AND weatherdate = '2021-10-22'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 9. Your company would like to now allow users to find cities where it is \n",
    "# raining on a specific date. Specifically, they would like a query to \n",
    "# show the city and state name, date, condition, and description for \n",
    "# only those cities where it is raining on the given date. \n",
    "# Write this query in Spark or Spark SQL. Which Cassandra filters \n",
    "# are used? Show with explain and highlight in your screenshot.\n",
    "query = '''\n",
    "SELECT city, state, condition, description, temperature_day \n",
    "FROM daily_city_weather \n",
    "WHERE condition = 'Rain' AND weatherdate = '2021-10-22'\n",
    "'''\n",
    "spark.sql(query).explain()\n",
    "print(f\"The query is {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c742e65-187a-41bb-baa6-424d6551458d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
