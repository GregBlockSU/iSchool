probTable <- table(rbinom(100000,9,.5))
cumsum(probTable)
barplot(cumsum(probTable), main =NULL)
probTable2 <- table(rbinom(100000,9,.5))/100000
cumsum(probTable2)
barplot(cumsum(probTable2), main =NULL)
#Page 36
#Exercise 6
test <- matrix(c(33, 47, 17, 3), ncol=2, byrow=TRUE)
colnames(test) <- c("High School Student","College Student")
rownames(test) <- c("Passed","Failed")
test <- as.table(test)
test
margin.table(test)
#Exercise 7
mortgage <- matrix(c(2, 69, 93933, 5996), ncol=2, byrow=TRUE)
colnames(mortgage) <- c("Pass","Fail")
rownames(mortgage) <- c("Default", "No Default")
mortgage <- as.table(mortgage)
mortgage
margin.table(mortgage)
93933/100000
#Exercise 8
mortgageFail <- mortgage/margin.table(mortgage)
mortgageFail
help("as.dfm")
??as.dfm
library(quanteda)
# first, remove the file and author names for a word cloud gallery
FedPapers_wc <- as.matrix(as.dfm(FederalistPapers)) #FederalistPapers[,3:72]
# setwd("C:/Users/HQRGRS27/OneDrive/Courses/Syracuse/IST707/Homework/04/")
FederalistPapers <- read.csv("fedPapers85.csv", row.names = 2, na.strings = c(""))
help("kmeans")
library(cluster)
library(factoextra)
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs <- Eurojobs[,1:2,]
colnames(Eurojobs) <- c("Agriculture", "Mining")
plot(Eurojobs$Mining ~ Eurojobs$Agriculture, xlab = "Agriculture", ylab="Mining", pch=19)
View(Eurojobs)
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs <- Eurojobs[1:2,]
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs <- Eurojobs[1:10, 1:2]
View(Eurojobs)
colnames(Eurojobs) <- c("Agriculture", "Mining")
plot(Eurojobs$Mining ~ Eurojobs$Agriculture, xlab = "Agriculture", ylab="Mining", pch=19)
set.seed(42)
model <- kmeans(Eurojobs, centers = 2)
print(model$cluster)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
model <- kmeans(Eurojobs, centers = 3)
print(model$cluster)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
View(Eurojobs)
model <- kmeans(Eurojobs, centers = 4)
print(model$cluster)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
model <- kmeans(Eurojobs, centers = 5)
print(model$cluster)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
model <- kmeans(Eurojobs, centers = 2)
print(model$cluster)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
model <- kmeans(Eurojobs, centers = 3)
print(model$cluster)
fviz_cluster(model, Eurojobs)
#########################
# Analyses and graphics for introductory slides
sampSize <- 100 # Set sample size
set.seed(11) # Control randomization
groupA <- rnorm(n=sampSize/2) # Generate one sample
groupA <- rnorm(n=sampSize/2) # Generate one sample
groupB <- rnorm(n=sampSize/2) + 0.2 # Generate a second sample that has an effect of 0.2
tout <- t.test(groupA, groupB) # Run a t-test
#########################
# Analyses and graphics for introductory slides
sampSize <- 100 # Set sample size
set.seed(11) # Control randomization
groupA <- rnorm(n=sampSize/2) # Generate one sample
groupB <- rnorm(n=sampSize/2) + 0.2 # Generate a second sample that has an effect of 0.2
tout <- t.test(groupA, groupB) # Run a t-test
tout$conf.int[1:2] # Report just the CI for showing to the class
# Now let's have a function that reports TRUE if
# both ends of the CI are below zero; remember that the because
# Group B mean should be larger than Group A mean, we are
# looking for a significant negative difference
rerunT <- function() {
groupA <- rnorm(n=sampSize/2)
groupB <- rnorm(n=sampSize/2) + 0.2
tout <- t.test(groupA, groupB)
(tout$conf.int[1] < 0) && (tout$conf.int[2] < 0)
}
# Create a table showing how many times we passed the test
table(replicate(n=10000,expr=rerunT()))
#install.packages("pwr")
library(pwr) # Do power analysis
pwr.t.test(n=(sampSize/2), d=0.2, type="two.sample") # What's the power to detect d=0.2?
# looking for a significant negative difference
rerunT <- function() {
# Here's a function to create a single mean difference
meanDiffs <- function() {
groupA <- rnorm(n=sampSize/2)
groupB <- rnorm(n=sampSize/2) + 0.2
mean(groupA) - mean(groupB)
}
# Show a histogram of the sampling distribution
hist(replicate(n=10000,expr=meanDiffs()))
# Show a histogram of the sampling distribution
hist(replicate(n=10000,expr=meanDiffs()))
# Run 100 t-tests and plot a CI for each one
df <- data.frame(Run = 1:100, U = 0, M = 0, L = 0)
for (i in 1:100) {
groupA <- rnorm(n=sampSize*100/2) # Sample one group
groupB <- rnorm(n=sampSize*100/2) + 0.2 # Sample a second group with d=0.2
tout <- t.test(groupA, groupB) # Run the t-test
df$L[i] <- tout$conf.int[1] # Lower limit of CI
df$U[i] <- tout$conf.int[2] # Upper limit of CI
df$M[i] <- mean(tout$conf.int) # Middle of CI
}
# Create a plot: How many overlap with 0?
# How many overlap with
require("ggplot2")
ggplot(df, aes(x = Run, y = M)) +
geom_point(size = 1) +
geom_errorbar(aes(ymax = U, ymin = L), width=0.15) +
geom_hline(col="red", yintercept = 0) +
geom_hline(col="green", yintercept = -0.2)
source('~/.active-rstudio-document')
source('~/.active-rstudio-document')
sampSize <- 100
set.seed(11)
groupA <- rnorm(n=sampSize/2)
groupB <- rnorm(n=sampSize/2) + 0.2
tout <- t.test(groupA, groupB)
rerunT <- function() {
groupA <- rnorm(n=sampSize/2)
groupB <- rnorm(n=sampSize/2) + 0.2
tout <- t.test(groupA, groupB)
(tout$conf.int[1] < 0) && (tout$conf.int[2] < 0)
}
table(replicate(n=10000,expr=rerunT()))
Sys.getenv("JAVA_HOME")
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_291/jre”)
Sys.setenv(JAVA_HOME="C:/Program Files/Java/jre1.8.0_291/jre")
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jre1.8.0_291\\jre")
Sys.getenv("JAVA_HOME")
library(RWeka)
Sys.setenv(JAVA_HOME="C:\\Program Files\\Java\\jre1.8.0_291")
library(RWeka)
trainset <- read.csv("titanic-train.csv")
setwd("C:/Users/HQRGRS27/source/git/SU/iSchool/ist707/Homework/05")
trainset <- read.csv("titanic-train.csv")
testset <- read.csv("titanic-test.csv")
NN <- make_Weka_filter("weka/filters/unsupervised/attribute/NumericToNominal")
trainset <- NN(data=trainset, control= Weka_control(R="1-3"), na.action = NULL)
testset <- NN(data=testset, control= Weka_control(R="1,3"), na.action = NULL)
MS <- make_Weka_filter("weka/filters/unsupervised/attribute/ReplaceMissingValues")
trainset <-MS(data=trainset, na.action = NULL)
testset <-MS(data=testset, na.action = NULL)
m=J48(Survived~., data = trainset, control=Weka_control(U=FALSE, M=2, C=0.5))
View(trainset)
trainset$Sex <- as.factor(trainset$Sex)
trainset$Ticket <- as.factor(trainset$Ticket)
trainset$Cabin <- as.factor(trainset$Cabin)
trainset$Cabin <- as.factor(trainset$Cabin)
trainset$Embarked <- as.factor(trainset$Embarked)
trainset <-MS(data=trainset, na.action = NULL)
testset <-MS(data=testset, na.action = NULL)
m=J48(Survived~., data = trainset, control=Weka_control(U=FALSE, M=2, C=0.5))
e <- evaluate_Weka_classifier(m, numFolds = 10, seed = 1, class = TRUE)
pred=predict (m, newdata = testset, type = c("class"))
myids=c("PassengerId")
id_col=testset[myids]
newpred=cbind(id_col, pred)
id_col=testset[myids]
newpred=cbind(id_col, pred)
colnames(newpred)=c("Passengerid", "Survived")
write.csv(newpred, file="titanic-J48-pred-05-17-2017.csv", row.names=FALSE)
write.csv(newpred, file="titanic-J48-pred-05-17-2017.csv", row.names=FALSE)
myVars=c("Pclass", "Sex", "Age", "SibSp", "Parch", "Fare", "Survived")
newtrain=trainset[myVars]
newtest=testset[myVars]
m=J48(Survived~., data = newtrain)
m=J48(Survived~., data = newtrain, control=Weka_control(U=FALSE, M=2, C=0.5))
e=evaluate_Weka_classifier(m, seed=1, numFolds=10)
pred=predict (m, newdata = newtest, type = c("class"))
myids=c("PassengerId")
id_col=testset[myids]
newpred=cbind(id_col, pred)
colnames(newpred)=c("Passengerid", "Survived")
View(newpred)
write.csv(newpred, file="titanic-J48-pred-05-17-2017-fs.csv", row.names=FALSE)
library(tm)
library(stringr)
library(wordcloud)
library(stringi)
library(Matrix)
library(tidytext)
library(dplyr)
library(ggplot2)
library(factoextra)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
(numberFedPapers<-length(FedPapersCorpus))
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
setwd("C:/Users/HQRGRS27/source/git/SU/iSchool/ist707/Homework/04/FedPapersCorpus")
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
(numberFedPapers<-length(FedPapersCorpus))
setwd("C:/Users/HQRGRS27/source/git/SU/iSchool/ist707/Homework/04")
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
(numberFedPapers<-length(FedPapersCorpus))
## The following will show you that you read in all the documents
(summary(FedPapersCorpus))
(meta(FedPapersCorpus[[1]]))
(meta(FedPapersCorpus[[1]],5))
(meta(FedPapersCorpus[[1]]))
class(FedPapersCorpus)
class(FedPapersCorpus[[1]])
(meta(FedPapersCorpus[[99]]))
test <- 1
(meta(test))
test <- DirSource("FedPapersCorpus")
(meta(test))
# Make sure JAGS is installed for native OS
# along with these packages:
#install.packages("rjags")
#install.packages("MCMCvis")
library("rjags")
# Make sure JAGS is installed for native OS
# along with these packages:
install.packages("rjags")
install.packages("rjags")
install.packages("MCMCvis")
library("rjags")
# Here's where we set up the data that we want to model.
# You can change this to contain two independent groups
# from any data source.
x <- mtcars$mpg[mtcars$am==0]
y <- mtcars$mpg[mtcars$am==1]
# Set up the priors
mean_mu = mean(c(x, y)) # Set the means
precision_mu = 1 / (mad( c(x, y) )^2 * 1000000) # Precision is inverse of variance
sigmaLow = mad( c(x, y) ) / 1000 # mad is mean absolute deviation
sigmaHigh = mad( c(x, y) ) * 1000
# This makes a list and then provides initial
# values for all of the elements in the JAGS
# model. Compare with the model file.
inits_list <- list(
mu_x = mean(x), mu_y = mean(y),
sigma_x = mad(x), sigma_y = mad(y),
nuMinusOne = 4)
# This makes a list and then copies in all of
# the data elements that the JAGS model needs
# in order to run.
data_list <- list(
x = x, y = y,
mean_mu = mean_mu,
precision_mu = precision_mu,
sigmaLow = sigmaLow,
sigmaHigh = sigmaHigh)
# These are the parameters we want to monitor.
# JAGS will create code to maintain a record of each of
# these variables for every MCMC sample drawn.
params <- c("mu_x", "mu_y", "mu_diff", "sigma_x", "sigma_y", "sigma_diff",
"nu", "eff_size", "x_pred", "y_pred")
# Run the model using the JAGS code from the text file.
model <- jags.model("two_indep_means.jags.txt", data = data_list,
inits = inits_list, n.chains = 3, n.adapt=1000)
setwd("C:/Users/HQRGRS27/source/git/SU/iSchool/ist772/Homework/05")
# Run the model using the JAGS code from the text file.
model <- jags.model("two_indep_means.jags.txt", data = data_list,
inits = inits_list, n.chains = 3, n.adapt=1000)
setwd("C:/Users/HQRGRS27/source/git/SU/iSchool/ist772/InClass/05")
# Run the model using the JAGS code from the text file.
model <- jags.model("two_indep_means.jags.txt", data = data_list,
inits = inits_list, n.chains = 3, n.adapt=1000)
View(model) # Optionally review the contents of the model object.
# This first step runs the burn-in samples, while
# the second step continues the model run with the
# post burn-in iterations.
update(model, 1000) # You can change the length of the burn-in samples
samples <- coda.samples(model, params, n.iter=10000) # You can increase the MCMC iterations.
# Inspecting the posterior
par(mar=c(1,1,1,1))
plot(samples, trace=F) # Give a density plot
plot(samples, density=F) # Give a trace plot; takes a while!
summary(samples) # Summrize the run
# Inspecting the posterior
par(mar=c(1,1,1,1))
plot(samples, trace=F) # Give a density plot
plot(samples, density=F) # Give a trace plot; takes a while!
summary(samples) # Summrize the run
# The bandwidth is calculated by 1.06 times the
# minimum of the standard deviation and the interquartile range
# divided by 1.34 times the sample size to the negative one fifth power
plot(samples[,"mu_diff"]) # Give the trace and density plots for mu_diff
# Make sure JAGS is installed for native OS
# along with these packages:
# install.packages("rjags")
# install.packages("MCMCvis")
library("rjags")
set.seed(1)
n <- 100
beta.0 <- 0 # This is the center of the prior for the mean
sigma.sq <- 5 # This is the prior for the variance
y <- rnorm(n, beta.0, sqrt(sigma.sq)) # Make some data to model with those parms
hist(y); mean(y); sd(y); var(y)
data <- list(y = y, n = n) # Prepare data to go to jags
inits <- list(beta.0 = beta.0, sigma.sq = sigma.sq) # Priors for the run
# n.chains - Number of separate MCMC chains to run
# n.adapt - Number of initial samples not counted in chain (inital samples may be used to set
# step size, e.g., for Metropolis-Hastings)
jags.m <- jags.model(file = "simpleDist.jags", data = data,
inits = inits, n.chains = 3, n.adapt = 100)
# This is a complex object full of code. The rest of the work is
# done in R.
View(jags.m)
# These are the parameters for which we want to see output.
params <- c("beta.0", "sigma.sq")
samps <- coda.samples(jags.m, params, n.iter = 10000)
# Note that the trace plots show separate colors for each chain.
# The chains should "mix" - that is they should converge with each
# other from a distributional standpoint.
plot(samps)
# These commands give diagnostics on the quality of the separate chains.
gelman.diag(samps) # Factors of one mean that between-chain and within-chain variance are equal
gelman.plot(samps)
burn.in <- 1000 # Provides an initial set of samples that will be ignored
summary(window(samps, start = burn.in))
library(MCMCvis) # Package for visualizing MCMC results
MCMCsummary(samps, round=3)
MCMCtrace(samps, pdf=FALSE)
x <- mtcars$mpg[mtcars$am==0]
y <- mtcars$mpg[mtcars$am==1]
# Set up the priors
mean_mu = mean(c(x, y)) # Set the means
precision_mu = 1 / (mad( c(x, y) )^2 * 1000000) # Precision is inverse of variance
sigmaLow = mad( c(x, y) ) / 1000 # mad is mean absolute deviation
sigmaHigh = mad( c(x, y) ) * 1000
inits_list <- list(
mu_x = mean(x), mu_y = mean(y),
sigma_x = mad(x), sigma_y = mad(y),
nuMinusOne = 4)
data_list <- list(
x = x, y = y,
mean_mu = mean_mu,
precision_mu = precision_mu,
sigmaLow = sigmaLow,
sigmaHigh = sigmaHigh)
# The parameters to monitor.
params <- c("mu_x", "mu_y", "mu_diff", "sigma_x", "sigma_y", "sigma_diff",
"nu", "eff_size", "x_pred", "y_pred")
# Run the model
model <- jags.model("two_indep_means.jags.txt", data = data_list,
inits = inits_list, n.chains = 3, n.adapt=1000)
update(model, 1000) # Length of the burn in samples
samples <- coda.samples(model, params, n.iter=10000)
summary(samples)
plot(samples[,"mu_diff"])
# Inspecting the posterior
plot(samples)
library(rjags)
x <- mtcars$mpg[mtcars$am==0]
y <- mtcars$mpg[mtcars$am==1]
# Set up the priors
mean_mu = mean(c(x, y)) # Set the means
precision_mu = 1 / (mad( c(x, y) )^2 * 1000000) # Precision is inverse of variance
sigmaLow = mad( c(x, y) ) / 1000 # mad is mean absolute deviation
sigmaHigh = mad( c(x, y) ) * 1000
inits_list <- list(
mu_x = mean(x), mu_y = mean(y),
sigma_x = mad(x), sigma_y = mad(y),
nuMinusOne = 4)
data_list <- list(
x = x, y = y,
mean_mu = mean_mu,
precision_mu = precision_mu,
sigmaLow = sigmaLow,
sigmaHigh = sigmaHigh)
# The parameters to monitor.
params <- c("mu_x", "mu_y", "mu_diff", "sigma_x", "sigma_y", "sigma_diff",
"nu", "eff_size")
# Run the model
model <- jags.model("simple_2_indep_means.jags.txt", data = data_list,
inits = inits_list, n.chains = 3, n.adapt=1000)
library(BayesFactor)
library(BEST)
set.seed(1) # Make sampling reproducible
x <- rexp(100, rate=1/10) # Exponential
y <- x + rnorm(100,mean=2) # Adds a random effect to y
hist(x); hist(y) # Take a look
t.test(x,y) # Conventional frequentist
bestOut <- BESTmcmc(x, y) # BEST estimate
bestOut <- BESTmcmc(x, y) # BEST estimate
plot(bestOut) # Plot of mean diff posterior
#Exercise 6
data("PlantGrowth")
PlantGrowth$weight[PlantGrowth$group=="ctrl"]
PlantGrowth$weight[PlantGrowth$group=="trt1"]
#Exercise 6
data("PlantGrowth")
(ctrl <- PlantGrowth$weight[PlantGrowth$group=="ctrl"])
(trt1 <- PlantGrowth$weight[PlantGrowth$group=="trt1"])
t.test(ctrl$weight,trt1$weight)
ctrl
(ctrl <- PlantGrowth[PlantGrowth$group=="ctrl"])
(ctrl <- PlantGrowth[PlantGrowth$group=="ctrl"])
#Exercise 6
data("PlantGrowth")
(ctrl <- PlantGrowth[PlantGrowth$group=="ctrl"])
View(PlantGrowth)
#Exercise 6
data("PlantGrowth")
(ctrl <- PlantGrowth$weight[PlantGrowth$group=="ctrl"])
(trt1 <- PlantGrowth$weight[PlantGrowth$group=="trt1"])
t.test(PlantGrowth$weight[PlantGrowth$group=="ctrl"] ,PlantGrowth$weight[PlantGrowth$group=="trt1"])
#Exercise 7
library(BEST)
PGBest <- BESTmcmc(PlantGrowth$weight[PlantGrowth$group=="ctrl"] ,PlantGrowth$weight[PlantGrowth$group=="trt1"])
plot(PGBest, main=NULL)
#Exercise 9
t.test(PlantGrowth$weight[PlantGrowth$group=="ctrl"] ,PlantGrowth$weight[PlantGrowth$group=="trt2"])
PGBest2 <- BESTmcmc(PlantGrowth$weight[PlantGrowth$group=="ctrl"] ,PlantGrowth$weight[PlantGrowth$group=="trt2"])
plot(PGBest2, main=NULL)
#Exercise 10
t.test(rnorm(100000,mean=17.1,sd=3.8),rnorm(100000,mean=17.2,sd=3.8))
#Exercise 6
data("PlantGrowth")
ctrl.weights = PlantGrowth$weight[PlantGrowth$group == 'ctrl']
trt1.weights = PlantGrowth$weight[PlantGrowth$group == 'trt1']
trt2.weights = PlantGrowth$weight[PlantGrowth$group == 'trt2']
# Perform a t-test between TRT1 and the control group.
t.test(ctrl.weights,trt1.weights)
t.test(PlantGrowth$weight[PlantGrowth$group=="ctrl"] ,PlantGrowth$weight[PlantGrowth$group=="trt1"])
#Exercise 7
library(rjags)
library(BEST)
PGBest <- BESTmcmc(ctrl.weights,trt1.weights)
plot(PGBest, main=NULL)
#Exercise 9
# Perform a t-test between TRT2 and the control group.
t.test(ctrl.weights,trt2.weights)
PGBest2 <- BESTmcmc(ctrl.weights,trt2.weights)
plot(PGBest2, main=NULL)
#Exercise 10
# Examine the results of running a t-test on a very large sample size.
t.test(rnorm(100000,mean=17.1,sd=3.8),rnorm(100000,mean=17.2,sd=3.8))
#Exercise 10
# Examine the results of running a t-test on a very large sample size.
set.seed(1234)
t.test(rnorm(100000,mean=17.1,sd=3.8),rnorm(100000,mean=17.2,sd=3.8))
#Exercise 10
# Examine the results of running a t-test on a very large sample size.
set.seed(1234)
t.test(rnorm(100000,mean=17.1,sd=3.8),rnorm(100000,mean=17.2,sd=3.8))
# Perform a t-test between TRT1 and the control group.
t.test(ctrl.weights,trt1.weights)
#Exercise 9
# Perform a t-test between TRT2 and the control group.
t.test(ctrl.weights,trt2.weights)
#Exercise 6
data("PlantGrowth")
ctrl.weights = PlantGrowth$weight[PlantGrowth$group == 'ctrl']
trt1.weights = PlantGrowth$weight[PlantGrowth$group == 'trt1']
trt2.weights = PlantGrowth$weight[PlantGrowth$group == 'trt2']
# Perform a t-test between TRT1 and the control group.
t.test(ctrl.weights,trt1.weights)
# Exercise 7
# Run a mcmc simulation with the control group and treatment group weights.
library(rjags)
library(BEST)
PGBest <- BESTmcmc(ctrl.weights,trt1.weights)
plot(PGBest, main=NULL)
#Exercise 9
# Perform a t-test between TRT2 and the control group.
t.test(ctrl.weights,trt2.weights)
PGBest2 <- BESTmcmc(ctrl.weights,trt2.weights)
plot(PGBest2, main=NULL)
#Exercise 10
# Examine the results of running a t-test on a very large sample size.
set.seed(1234)
t.test(rnorm(100000,mean=17.1,sd=3.8),rnorm(100000,mean=17.2,sd=3.8))
install.packages("rjags")
install.packages("rjags")
install.packages("rjags")
install.packages("rjags")
install.packages("rjags")
install.packages("rjags")
install.packages("rjags")
library("BEST")
# Exercise 7
# Run a mcmc simulation with the control group and treatment group weights.
library(rjags)
library(BEST)
# The reported value t is 1.1913, the degrees of freedom is 16.524 and the p-value is
# 0.2504. The confidence interval is -0.2875162 to 1.0295162. With an alpha of .05 and
# a p-value of 0.2504, we would fail to reject the null hypothesis.
install.packages("rjags")
install.packages("rjags")
