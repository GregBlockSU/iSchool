boxplot(weight ~ Time, data=ChickWeight)
# t-test
ch16index <- ChickWeight$Time == 16 # Chicks measured at time 16
ch18index <- ChickWeight$Time == 18 # Chicks measured at time 18
bothChicks <- ChickWeight[ch16index | ch18index,] # Both sets together
time16weight <- bothChicks[bothChicks$Time == 16,"weight"] # Grab the weights for t=16
time18weight <- bothChicks[bothChicks$Time == 18,"weight"] # Grab the weights for t=18
cor(time16weight,time18weight) # Are they correlated?
mean(time16weight)
mean(time18weight)
t.test(time18weight,time16weight,paired = FALSE) # Independent groups t-test
BESTmcmc(time18weight,time16weight)# Run the Bayesian equivalent
library(MCMCpack)
BESTmcmc(time18weight,time16weight)# Run the Bayesian equivalent
library(BEST)
BESTmcmc(time18weight,time16weight)# Run the Bayesian equivalent
t.test(time18weight,time16weight,paired = TRUE) # Dependent groups t-test
weightDiffs <- time18weight - time16weight # Make difference scores
t.test(weightDiffs) # Run a one sample t-test on difference scores
BESTmcmc(weightDiffs) # Run the Bayesian equivalent on difference scores
# ANOVA within
chwBal <- ChickWeight # Copy the dataset
chwBal$TimeFact <- as.factor(chwBal$Time) # Convert Time to a factor
list <- rowSums(table(chwBal$Chick,chwBal$TimeFact))==12 # Make a list of rows
list <- list[list==TRUE] # Keep only those with 12 observations
list <- as.numeric(names(list)) # Extract the row indices
chwBal <- chwBal[chwBal$Chick %in% list,] # Match against the data
table(chwBal$Chick,chwBal$TimeFact) # Check results
summary(aov(weight ~ TimeFact + Error(Chick), data=chwBal))
# Code for the Sidebar
install.packages("ez")
library("ez")
ezANOVA(data=chwBal, dv=.(weight), within=.(TimeFact), wid=.(Chick), detailed=TRUE)
set.seed(1234)				# Control random numbers
tslen <- 180					# About half a year of daily points
ex1 <- rnorm(n=tslen,mean=0,sd=10)        # Make a random variable
tex1 <- ex1 + seq(from=1, to=tslen, by=1) # Add the fake upward trend
plot.ts(tex1) # Plot the time series with a connected line
ex2 <- rnorm(n=tslen,mean=0,sd=10)        # Make another random variable
tex2 <- ex2 + seq(from=1, to=tslen, by=1) # Add the fake upward trend
cor(ex1, ex2)                           # Correlation between the two random variables
plot(tex1, tex2)
cor(tex1, tex2)                         # Correlation between the two time series
ex3 <- rnorm(n=tslen,mean=0,sd=10)
tex3 <- ex3 + seq(from=1, to=tslen, by=1) # Add the fake upward trend
tex3 <- tex3 + sin(seq(from=0,to=36,length.out=tslen))*20
plot.ts(tex3)
decOut <- decompose(ts(tex3,frequency=30))
plot(decOut)
mean(decOut$trend,na.rm=T)
mean(decOut$seasonal)
mean(decOut$random,na.rm=T)
cor(ex3, decOut$random, use="complete.obs")
set.seed(1234)
tslen <- 180
ex1 <- rnorm(n=tslen,mean=0,sd=10)        # Make a random variable
acf(ex1, main=NULL)
tex1 <- ex1 + seq(from=1, to=tslen, by=1) # Add the fake upward trend
acf(tex1, main=NULL)
ex3 <- rnorm(n=tslen,mean=0,sd=10)
tex3 <- ex3 + seq(from=1, to=tslen, by=1) # Add the fake upward trend
tex3 <- tex3 + sin(seq(from=0,to=36,length.out=tslen))*20
acf(tex3)
acf(decOut$trend,na.action=na.pass)
acf(decOut$seasonal, main=NULL)
acf(decOut$random,na.action=na.pass, main=NULL)
install.packages("tseries")
# install.packages("tseries")
library("tseries")
decComplete <- decOut$random[complete.cases(decOut$random)]
adf.test(decComplete) # Shows significant, so it is stationary
plot(EuStockMarkets, main=NULL)
plot(diff(EuStockMarkets), main=NULL)
# The following code examines change point analysis
install.packages("changepoint")
# The following code examines change point analysis
# install.packages("changepoint")
library(changepoint)
# Use changepoint analysis to locate the positon of a mean change
DAX <- EuStockMarkets[,1]
DAXcp <- cpt.mean(DAX)
DAXcp
cpt.var(diff(EuStockMarkets[,"DAX"])) # Examine the change in variance
# Change to a simple output data structure to retrieve the confidence value
DAXcp <- cpt.mean(DAX,class=FALSE)
DAXcp["conf.value"]
# Now difference the DAX series and look for a change in variance
dEUstocks <- diff(EuStockMarkets)
plot(dEUstocks)
dDAX <- dEUstocks[,1]
dDAXcp <- cpt.var(dDAX)
plot(dDAXcp,cpt.col="grey",cpt.width=5)
dDAXcp
install.packages("bcp")
# install.packages("bcp")
library(bcp)
bcpDAX <- bcp(as.vector(DAX))
plot(bcpDAX,outer.margins = list(left = unit(4,"lines"), bottom = unit(3, "lines"), right = unit(3, "lines"), top = unit(2,"lines")), main=NULL)
plot(bcpDAX$posterior.prob>.95)
install.packages("drat", repos = "https://cran.rstudio.com")
install.packages("mxnet")
library(mxnet)
# drat::addRepo("dmlc")
install.packages("mxnet", repos = "https://cran.rstudio.com")
drat::addRepo("dmlc")
install.packages("mxnet")
setwd("C:/Users/HQRGRS27/source/git/SU/iSchool/ist707/Homework/06")
train <- data.matrix(read.csv("digit_train.csv", header=T))
test <- data.matrix(read.csv("digit_test.csv", header=T))
dim(train[,-1]) #to remove the 'label' column
dim(test)
barplot(table(train[,1]), col=rainbow(10, 0.5), main="n Digits in Train")
# There is around 4000 observations for each digit. Each row has 784 columns
# (pixels) which form a 28x28 image. Let's see what the handwritten digits look
# like by plotting them. Here is a function to plot a selection of digits from
# the train dataset.
plotTrain <- function(images){
op <- par(no.readonly=TRUE)
x <- ceiling(sqrt(length(images)))
par(mfrow=c(x, x), mar=c(.1, .1, .1, .1))
for (i in images){ #reverse and transpose each matrix to rotate images
m <- matrix(train[i,-1], nrow=28, byrow=TRUE)
m <- apply(m, 2, rev)
image(t(m), col=grey.colors(255), axes=FALSE)
text(0.05, 0.2, col="white", cex=1.2, train[i, 1])
}
par(op) #reset the original graphics parameters
}
# Now let's use this function to look at the first 36 images. You can look at
# many images if you wanted too, e.g., plotTrain(1001:1100)
plotTrain(1:36)
train.x <- train[,-1] #remove 'label' column
train.y <- train[,1] #label column
train.x <- t(train.x/255)
test.x <- t(test/255)
# The first model is basic network that uses three Fully Connected layers, each
# with 128, 64, and 10 neurons, respectively. These layers are connected by 2
# Activation Layers, which use the **ReLU** activation function. You can
# experiment how the number of layers and neurons change the predictive ability
# of the model. It is common to make the number of neurons in the last layer
# equal to the number of classes (in this case, 10 digits). For more
# information, I recommend checking out the [MXNet tutorial
# page](http://mxnet.io).
m1.data <- mx.symbol.Variable("data") # Notice how each layer is passed to the next
#MXNet isn't available directly from CRAN, so uncomment the first 3 lines to
#install it from the dmlc repo (developers of XGBoost). install.packages("drat",
#repos = "https://cran.rstudio.com") drat::addRepo("dmlc")
#install.packages("mxnet")
library(mxnet)
#MXNet isn't available directly from CRAN, so uncomment the first 3 lines to
#install it from the dmlc repo (developers of XGBoost).
install.packages("drat", repos = "https://cran.rstudio.com")
install.packages("drat", repos = "https://cran.rstudio.com")
drat::addRepo("dmlc")
install.packages("mxnet")
library(nnet)
train_orig <- data.matrix(read.csv("digit_train.csv", header=T))
# save the training labels
train_orig_labels <- train_orig[, 1]
train_orig_labels <- as.factor(train_orig_labels$label)
library(nnet)
train_orig <- data.matrix(read.csv("digit_train.csv", header=T))
View(train_orig)
train_orig <- read.csv("digit_train.csv", header=T)
test_orig <- read_csv("digit_test.csv", header=T)
library(readr)
test_orig <- read_csv("digit_test.csv", header=T)
test_orig <- read_csv("digit_test.csv")
train_orig <- read_csv("digit_train.csv")
test_orig <- read_csv("digit_test.csv")
# save the training labels
train_orig_labels <- train_orig[, 1]
train_orig_labels <- as.factor(train_orig_labels$label)
summary(train_orig_labels)
library(randomForest)
# split the training data into train and test to do local evaluation
set.seed(123)
rows <- sample(1:nrow(train_orig), as.integer(0.7*nrow(train_orig)))
# Get train and test labels
train_labels <- train_orig[rows, 1]
test_labels <- train_orig[-rows, 1]
# convert the labels to factors
train_labels <- as.factor(train_labels$label)
test <- data.matrix(read.csv("digit_test.csv", header=T))
dim(train[,-1]) #to remove the 'label' column
# custom normalization function
normalize <- function(x) {
return(x / 255)
}
# create the train and test datasets and apply normalization
train_norm <- as.data.frame(lapply(train_orig[rows, -1], normalize))
test_norm <- as.data.frame(lapply(train_orig[-rows,-1], normalize))
# check a random pixel to see if the normalization worked
summary(train_orig$pixel350)
# check a random pixel to see if the normalization worked
summary(train_norm$pixel350)
summary(train_norm$pixel350)
# check a random pixel to see if the normalization worked
summary(train_norm$pixel350)
# check a random pixel to see if the normalization worked
summary(train_orig$pixel350)
summary(train_norm$pixel350)
summary(test_norm$pixel350)
# create the class indicator matrix
train_labels_matrix = class.ind(train_labels)
head(train_labels)
head(train_labels_matrix)
nn = nnet(train_norm, train_labels_matrix, size = 1, softmax = TRUE)
## iter  30 value 55912.804141
## iter  40 value 54648.757612
## iter  50 value 53950.781576
## iter  60 value 52927.199756
## iter  70 value 52291.634751
## iter  80 value 51967.602466
## iter  90 value 51774.654787
## iter 100 value 51643.951402
## final  value 51643.951402
## stopped after 100 iterations
proc.time() - startTime
##    user  system elapsed
##   46.97    0.13   47.54
nn
pred = predict(nn, test_norm, type="class")
cbind(head(pred), head(test_labels))
# evaluate the model
accuracy <- mean(pred == test_labels)
print(paste('Accuracy:', accuracy))
barplot(table(train_orig[,1]), col=rainbow(10, 0.5), main="n Digits in Train")
numTrees <- 25
# Train on entire training dataset and predict on the test
startTime <- proc.time()
rf <- randomForest(train_orig[-1], train_orig_labels, xtest=test_orig,
ntree=numTrees)
proc.time() - startTime
print(rf)
# output predictions for submission
predictions <- data.frame(ImageId=1:nrow(test_orig),
Label=levels(train_orig_labels)[rf$test$predicted])
head(predictions)
# There is around 4000 observations for each digit. Each row has 784 columns
# (pixels) which form a 28x28 image. Let's see what the handwritten digits look
# like by plotting them. Here is a function to plot a selection of digits from
# the train dataset.
plotTrain <- function(images){
op <- par(no.readonly=TRUE)
x <- ceiling(sqrt(length(images)))
par(mfrow=c(x, x), mar=c(.1, .1, .1, .1))
for (i in images){ #reverse and transpose each matrix to rotate images
m <- matrix(train_orig[i,-1], nrow=28, byrow=TRUE)
m <- apply(m, 2, rev)
image(t(m), col=grey.colors(255), axes=FALSE)
text(0.05, 0.2, col="white", cex=1.2, train_orig[i, 1])
}
par(op) #reset the original graphics parameters
}
# Now let's use this function to look at the first 36 images. You can look at
# many images if you wanted too, e.g., plotTrain(1001:1100)
plotTrain(1:36)
# There is around 4000 observations for each digit. Each row has 784 columns
# (pixels) which form a 28x28 image. Let's see what the handwritten digits look
# like by plotting them. Here is a function to plot a selection of digits from
# the train dataset.
plotTrain <- function(images){
op <- par(no.readonly=TRUE)
x <- ceiling(sqrt(length(images)))
par(mfrow=c(x, x), mar=c(.1, .1, .1, .1))
for (i in images){ #reverse and transpose each matrix to rotate images
m <- matrix(data.matrix(train_orig[i,-1]), nrow=28, byrow=TRUE)
m <- apply(m, 2, rev)
image(t(m), col=grey.colors(255), axes=FALSE)
text(0.05, 0.2, col="white", cex=1.2, train_orig[i, 1])
}
par(op) #reset the original graphics parameters
}
# Now let's use this function to look at the first 36 images. You can look at
# many images if you wanted too, e.g., plotTrain(1001:1100)
plotTrain(1:36)
accuracy <- mean(predictions == test_labels)
View(test_labels)
View(predictions)
head(predictions)
View(test_labels)
View(test_orig)
View(predictions)
View(train_orig)
head(predictions)
View(predictions)
View(test_orig)
rotate <- function(x) t(apply(x, 2, rev)) # reverses (rotates the matrix)
rotate <- function(x) t(apply(x, 2, rev)) # reverses (rotates the matrix)
par(mfrow=c(2,3)) # Plotting in 2*3 format (random forest)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(orig_test[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=prediction[x,2]
)
)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=prediction[x,2]
)
)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=predictions[x,2]
)
)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=predictions[x,2]
)
)
par(mfrow=c(2,3)) # Plotting in 2*3 format (random forest)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=predictions[x,2]
)
)
View(predictions)
head(predictions)
pred = predict(nn, test_norm, type="class")
cbind(head(pred), head(test_labels))
# evaluate the model
accuracy <- mean(pred == test_labels)
print(paste('Accuracy:', accuracy))
par(mfrow=c(2,3)) # Plotting in 2*3 format (random forest)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=pred[x,2]
)
)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=pred[x]
)
)
par(mfrow=c(2,3)) # Plotting in 2*3 format (neural net)
lapply(1:6,
function(x) image( #norow = 28 because this is 28 pixel image
rotate(matrix(unlist(test_orig[x,]),nrow = 28,byrow = T)),
col=grey.colors(255),
xlab=pred[x]
)
)
