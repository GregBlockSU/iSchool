# Add the standard deviation of that sampling distribution to the list
sdVector <- c(sdVector, sd(samplingDistribution))
}
plot(sampSizes, sdVector)
# Run these three functions to get a clean test of homework code
#dev.off() # Clear the graph window
cat('\014')  # Clear the console
rm(list=ls()) # Clear user objects from the environment
# Load the data set
data("ChickWeight")
# Generate summary information on the data set
summary(ChickWeight)
# Report the dimensions of the data set
dim(ChickWeight)
? ChickWeight
# Report on a summary of the weight variable.
summary(ChickWeight$weight)
# Return the first six values of the weight
# variable in the data set.
head(ChickWeight$weight)
# Return the mean of the weight attribute
mean(ChickWeight$weight)
# Pass the weights to a variable
myChkWts <- ChickWeight$weight
# Return the median weight value
quantile(myChkWts, 0.5)
# Create a histogram from the weights vector
hist(myChkWts, main="Histogram of Chick Weights"
, ylab = "frequency"
, xlab="grams"
, col="darkgoldenrod3"
, breaks=20)
abline(v=quantile(myChkWts,c(.025,.975)),col="darkgreen",lwd=2)
# Return quantile values
quantile(myChkWts,c(.025,.50,.975))
mean(myChkWts)
# Create a collection of sample means from the weight attribute.
samplingDistribution <- replicate(10000,mean(sample(myChkWts,11,replace=T)))
# Display a histogram of the sampling distribution
hist(samplingDistribution, main="Histogram of Chick Weight Sample Means"
, ylab = "frequency"
, xlab="average weight (gms)"
, col="darkslateblue"
, border = "white"
, breaks=30)
abline(v=quantile(samplingDistribution,c(.025,.975)),col="red3",lwd=2)
# Return quantile values
quantile(samplingDistribution, c(.025, .975))
# Create a collection of sample means from the weight attribute.
samplingDistribution2 <- replicate(10000,mean(sample(myChkWts,100,replace=T)))
# Display a histogram of the sampling distribution
hist(samplingDistribution2, main="Histogram of Chick Weight Sample Means (n=100)"
, ylab = "frequency"
, xlab="average weight (gms)"
, col="darkgreen"
, border = "white"
, breaks=30)
abline(v=quantile(samplingDistribution2,c(.025,.975)),col="darkorange",lwd=2)
# Return quantile values
quantile(samplingDistribution2, c(.025, .975))
set.seed(1234) 			      # Control randomization
testPop <- rnorm(100000, mean=100, sd=10) # Create simulated pop
# Custom function to pull one sample
sampleTestScores <- function(n){sample(testPop,size=n,replace=TRUE)}
samplingDistribution <- replicate(1000, mean(sampleTestScores(100)))
# par(mfrow=c(2,1))
hist(testPop, xlim=c(50,140))
hist(samplingDistribution, xlim=c(50,140))
par(mfrow=c(1,1))
# Case Study A
hist(samplingDistribution)
abline(v=quantile(samplingDistribution,prob=0.025))
abline(v=quantile(samplingDistribution,prob=0.975))
abline(v=101, col="red")
# Case Study B
samplingDistribution <- replicate(1000, mean(sampleTestScores(49)))
hist(samplingDistribution)
abline(v=quantile(samplingDistribution,prob=0.025))
abline(v=quantile(samplingDistribution,prob=0.975))
abline(v=104, col="red")
# Case Study C
samplingDistribution <- replicate(1000, mean(sampleTestScores(500)))
hist(samplingDistribution)
abline(v=quantile(samplingDistribution,prob=0.025))
abline(v=quantile(samplingDistribution,prob=0.975))
abline(v=101, col="red")
# Case Study D
testPop <- rnorm(100000, mean=50, sd=5) # Create simulated pop
samplingDistribution <- replicate(1000, mean(sampleTestScores(64)))
hist(samplingDistribution)
abline(v=quantile(samplingDistribution,prob=0.025))
abline(v=quantile(samplingDistribution,prob=0.975))
abline(v=48, col="red")
# Case Study E
samplingDistribution <- replicate(1000, (mean(sampleTestScores(36))-mean(sampleTestScores(36))))
hist(samplingDistribution)
abline(v=quantile(samplingDistribution,prob=0.025))
abline(v=quantile(samplingDistribution,prob=0.975))
abline(v=3, col="red")
sdVector <- NULL # Start a list with nothing
sdVector <- NULL # Start a list with nothing
sampSizes <- (2:10)^2 # A list of 9 sample sizes ranging from 4 to 100
for (i in sampSizes)
{
# Do a sampling distribution for each sample size
samplingDistribution <- replicate(1000, mean(sampleTestScores(i)))
# Add the standard deviation of that sampling distribution to the list
sdVector <- c(sdVector, sd(samplingDistribution))
}
plot(sampSizes, sdVector)
sdVector <- NULL # Start a list with nothing
sampSizes <- (2:15)^2 # A list of 9 sample sizes ranging from 4 to 100
for (i in sampSizes)
{
# Do a sampling distribution for each sample size
samplingDistribution <- replicate(1000, mean(sampleTestScores(i)))
# Add the standard deviation of that sampling distribution to the list
sdVector <- c(sdVector, sd(samplingDistribution))
}
plot(sampSizes, sdVector)
sdVector <- NULL # Start a list with nothing
sampSizes <- (2:20)^2 # A list of 9 sample sizes ranging from 4 to 100
for (i in sampSizes)
{
# Do a sampling distribution for each sample size
samplingDistribution <- replicate(1000, mean(sampleTestScores(i)))
# Add the standard deviation of that sampling distribution to the list
sdVector <- c(sdVector, sd(samplingDistribution))
}
plot(sampSizes, sdVector)
sdVector <- NULL # Start a list with nothing
sampSizes <- (2:100)^2 # A list of 9 sample sizes ranging from 4 to 100
for (i in sampSizes)
{
# Do a sampling distribution for each sample size
samplingDistribution <- replicate(1000, mean(sampleTestScores(i)))
# Add the standard deviation of that sampling distribution to the list
sdVector <- c(sdVector, sd(samplingDistribution))
}
plot(sampSizes, sdVector)
# Load the data set
data("ChickWeight")
# Generate summary information on the data set
summary(ChickWeight)
View(ChickWeight)
View(ChickWeight)
# Report the dimensions of the data set
dim(ChickWeight)
? ChickWeight
ChickWeight$weight
# Report on a summary of the weight variable.
summary(ChickWeight$weight)
# Return the first six values of the weight
# variable in the data set.
head(ChickWeight$weight)
# Return the mean of the weight attribute
mean(ChickWeight$weight)
# Pass the weights to a variable
myChkWts <- ChickWeight$weight
# Return the median weight value
quantile(myChkWts, 0.5)
# Create a histogram from the weights vector
hist(myChkWts, main="Histogram of Chick Weights"
, ylab = "frequency"
, xlab="grams"
, col="darkgoldenrod3"
, breaks=20)
abline(v=quantile(myChkWts,c(.025,.975)),col="darkgreen",lwd=2)
# Return quantile values
quantile(myChkWts,c(.025,.50,.975))
mean(myChkWts)
# Create a collection of sample means from the weight attribute.
samplingDistribution <- replicate(10000,mean(sample(myChkWts,11,replace=T)))
# Display a histogram of the sampling distribution
hist(samplingDistribution, main="Histogram of Chick Weight Sample Means"
, ylab = "frequency"
, xlab="average weight (gms)"
, col="darkslateblue"
, border = "white"
, breaks=30)
abline(v=quantile(samplingDistribution,c(.025,.975)),col="red3",lwd=2)
# Return quantile values
quantile(samplingDistribution, c(.025, .975))
# Create a collection of sample means from the weight attribute.
samplingDistribution2 <- replicate(10000,mean(sample(myChkWts,100,replace=T)))
# Display a histogram of the sampling distribution
hist(samplingDistribution2, main="Histogram of Chick Weight Sample Means (n=100)"
, ylab = "frequency"
, xlab="average weight (gms)"
, col="darkgreen"
, border = "white"
, breaks=30)
abline(v=quantile(samplingDistribution2,c(.025,.975)),col="darkorange",lwd=2)
# Return quantile values
quantile(samplingDistribution2, c(.025, .975))
# Generate summary information on the data set
summary(ChickWeight)
# page 35
#Exercise 1
table(rbinom(100000,9,.5))
hist(rbinom(100000,9,.5))
barplot(table(rbinom(100000,9,.5)), main = NULL)
#Exercise 2
probTable <- table(rbinom(100000,9,.5))
cumsum(probTable)
barplot(cumsum(probTable), main =NULL)
probTable2 <- table(rbinom(100000,9,.5))/100000
cumsum(probTable2)
barplot(cumsum(probTable2), main =NULL)
#Page 36
#Exercise 6
test <- matrix(c(33, 47, 17, 3), ncol=2, byrow=TRUE)
colnames(test) <- c("High School Student","College Student")
rownames(test) <- c("Passed","Failed")
test <- as.table(test)
test
margin.table(test)
#Exercise 7
mortgage <- matrix(c(2, 69, 93933, 5996), ncol=2, byrow=TRUE)
colnames(mortgage) <- c("Pass","Fail")
rownames(mortgage) <- c("Default", "No Default")
mortgage <- as.table(mortgage)
mortgage
margin.table(mortgage)
93933/100000
#Exercise 8
mortgageFail <- mortgage/margin.table(mortgage)
mortgageFail
help("as.dfm")
??as.dfm
library(quanteda)
# first, remove the file and author names for a word cloud gallery
FedPapers_wc <- as.matrix(as.dfm(FederalistPapers)) #FederalistPapers[,3:72]
# setwd("C:/Users/HQRGRS27/OneDrive/Courses/Syracuse/IST707/Homework/04/")
FederalistPapers <- read.csv("fedPapers85.csv", row.names = 2, na.strings = c(""))
help("kmeans")
library(tm)
library(stringr)
library(wordcloud)
library(stringi)
library(Matrix)
library(tidytext)
library(dplyr)
library(ggplot2)
library(factoextra)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
setwd("C:/Users/HQRGRS27/OneDrive/Courses/Syracuse/IST707/Homework/04")
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
(numberFedPapers<-length(FedPapersCorpus))
## The following will show you that you read in all the documents
(summary(FedPapersCorpus))
(meta(FedPapersCorpus[[1]]))
(meta(FedPapersCorpus[[1]],5))
#Data Preparation and Transformation on Fed Papers
##Remove punctuation,numbers, and space
(getTransformations())
(nFedPapersCorpus<-length(FedPapersCorpus))
##Ignore extremely rare words i.e. terms that appear in less then 1% of the documents
(minTermFreq <-30)
##Ignore overly common words i.e. terms that appear in more than 50% of the documents
(maxTermFreq <-1000)
(MyStopwords <- c("will","one","two", "may","less","publius","Madison","Alexand", "Alexander",
"James", "Hamilton","Jay", "well","might","without","small", "single", "several",
"but", "very", "can", "must", "also", "any", "and", "are", "however", "into", "almost",
"can","for", "add", "Author" ))
(STOPS <-stopwords('english'))
Papers_DTM <- DocumentTermMatrix(FedPapersCorpus,
control = list(
stopwords = TRUE,
wordLengths=c(3, 15),
removePunctuation = T,
removeNumbers = T,
tolower=T,
stemming = T,
remove_separators = T,
stopwords = MyStopwords,
removeWords=STOPS,
removeWords=MyStopwords,
bounds = list(global = c(minTermFreq, maxTermFreq))
))
##inspect FedPapers Document Term Matrix (DTM)
DTM <- as.matrix(Papers_DTM)
(DTM[1:11,1:10])
##Look at word frequencies
WordFreq <- colSums(as.matrix(Papers_DTM))
(head(WordFreq))
(length(WordFreq))
## [1] 427
ord <- order(WordFreq)
(WordFreq[head(ord)])
(WordFreq[tail(ord)])
(Row_Sum_Per_doc <- rowSums((as.matrix(Papers_DTM))))
## Create a normalized version of Papers_DTM
Papers_M <- as.matrix(Papers_DTM)
Papers_M_N1 <- apply(Papers_M, 1, function(i) round(i/sum(i),3))
Papers_Matrix_Norm <- t(Papers_M_N1)
## Convert to matrix and view
Papers_dtm_matrix = as.matrix(Papers_DTM)
str(Papers_dtm_matrix)
(Papers_dtm_matrix[c(1:11),c(2:10)])
# Below we label the data, prepare for modeling, and create some wordclouds
## Also convert to DF
Papers_DF <- as.data.frame(as.matrix(Papers_Matrix_Norm))
Papers_DF1<- Papers_DF%>%add_rownames()
## Warning: Deprecated, use tibble::rownames_to_column() instead.
names(Papers_DF1)[1]<-"Author"
Papers_DF1[1:11,1]="dispt"
Papers_DF1[12:62,1]="hamil"
Papers_DF1[63:77,1]="madis"
head(Papers_DF1)
#Wordcloud Visualization Hamilton, Madison and Disputed Papers
DisputedPapersWC<- wordcloud(main="Disputed Papers", colnames(Papers_dtm_matrix), Papers_dtm_matrix[11,])
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Title of my first plot")
#Wordcloud Visualization Hamilton, Madison and Disputed Papers
DisputedPapersWC<- wordcloud(main="Disputed Papers", colnames(Papers_dtm_matrix), Papers_dtm_matrix[11,])
#Wordcloud Visualization Hamilton, Madison and Disputed Papers
DisputedPapersWC<- wordcloud(main="Disputed Papers", colnames(Papers_dtm_matrix), Papers_dtm_matrix[11,])
(head(sort(as.matrix(Papers_dtm_matrix)[11,], decreasing = TRUE), n=50))
HamiltonPapersWC <-wordcloud(colnames(Papers_dtm_matrix),Papers_dtm_matrix[12:62,])
MadisonPapersHW <-wordcloud(colnames(Papers_dtm_matrix), Papers_dtm_matrix[63:77,])
setwd("C:/Users/HQRGRS27/OneDrive/Courses/Syracuse/IST707/Homework/04")
getwd()
library(tm)
library(stringr)
library(wordcloud)
library(stringi)
library(Matrix)
library(tidytext)
library(dplyr)
library(ggplot2)
library(factoextra)
library(rpart)
library(rattle)
library(rpart.plot)
library(RColorBrewer)
#Load Fed Papers Corpus
FedPapersCorpus <- Corpus(DirSource("FedPapersCorpus"))
(numberFedPapers<-length(FedPapersCorpus))
## The following will show you that you read in all the documents
(summary(FedPapersCorpus))
(meta(FedPapersCorpus[[1]]))
(meta(FedPapersCorpus[[1]],5))
#Data Preparation and Transformation on Fed Papers
##Remove punctuation,numbers, and space
(getTransformations())
(nFedPapersCorpus<-length(FedPapersCorpus))
##Ignore extremely rare words i.e. terms that appear in less then 1% of the documents
(minTermFreq <-30)
##Ignore overly common words i.e. terms that appear in more than 50% of the documents
(maxTermFreq <-1000)
(MyStopwords <- c("will","one","two", "may","less","publius","Madison","Alexand", "Alexander",
"James", "Hamilton","Jay", "well","might","without","small", "single", "several",
"but", "very", "can", "must", "also", "any", "and", "are", "however", "into", "almost",
"can","for", "add", "Author" ))
(STOPS <-stopwords('english'))
help("DocumentTermMatrix")
(STOPS <-stopwords('english'))
Papers_DTM <- DocumentTermMatrix(FedPapersCorpus,
control = list(
stopwords = TRUE,
wordLengths=c(3, 15),
removePunctuation = T,
removeNumbers = T,
tolower=T,
stemming = T,
remove_separators = T,
stopwords = MyStopwords,
removeWords=STOPS,
removeWords=MyStopwords,
bounds = list(global = c(minTermFreq, maxTermFreq))
))
##inspect FedPapers Document Term Matrix (DTM)
DTM <- as.matrix(Papers_DTM)
(DTM[1:11,1:10])
##Look at word frequencies
WordFreq <- colSums(as.matrix(Papers_DTM))
(head(WordFreq))
(length(WordFreq))
## [1] 427
ord <- order(WordFreq)
(WordFreq[head(ord)])
(WordFreq[tail(ord)])
(Row_Sum_Per_doc <- rowSums((as.matrix(Papers_DTM))))
## Create a normalized version of Papers_DTM
Papers_M <- as.matrix(Papers_DTM)
Papers_M_N1 <- apply(Papers_M, 1, function(i) round(i/sum(i),3))
Papers_Matrix_Norm <- t(Papers_M_N1)
## Convert to matrix and view
Papers_dtm_matrix = as.matrix(Papers_DTM)
str(Papers_dtm_matrix)
(Papers_dtm_matrix[c(1:11),c(2:10)])
# Below we label the data, prepare for modeling, and create some wordclouds
## Also convert to DF
Papers_DF <- as.data.frame(as.matrix(Papers_Matrix_Norm))
Papers_DF1<- Papers_DF%>%add_rownames()
## Warning: Deprecated, use tibble::rownames_to_column() instead.
names(Papers_DF1)[1]<-"Author"
Papers_DF1[1:11,1]="dispt"
Papers_DF1[12:62,1]="hamil"
Papers_DF1[63:77,1]="madis"
head(Papers_DF1)
layout(matrix(c(1, 2), nrow=2), heights=c(1, 4))
par(mar=rep(0, 4))
plot.new()
text(x=0.5, y=0.5, "Title of my first plot")
#Wordcloud Visualization Hamilton, Madison and Disputed Papers
DisputedPapersWC<- wordcloud(main="Disputed Papers", colnames(Papers_dtm_matrix), Papers_dtm_matrix[11,])
(head(sort(as.matrix(Papers_dtm_matrix)[11,], decreasing = TRUE), n=50))
HamiltonPapersWC <-wordcloud(colnames(Papers_dtm_matrix),Papers_dtm_matrix[12:62,])
MadisonPapersHW <-wordcloud(colnames(Papers_dtm_matrix), Papers_dtm_matrix[63:77,])
##Make Train and Test sets
trainRatio <- .60
##Make Train and Test sets
trainRatio <- .60
set.seed(11) # Set Seed so that same sample can be reproduced in future also
sample <- sample.int(n = nrow(Papers_DF1), size = floor(trainRatio*nrow(Papers_DF1)), replace = FALSE)
train <- Papers_DF1[sample, ]
test <- Papers_DF1[-sample, ]
# train / test ratio
length(sample)/nrow(Papers_DF1)
##Decision Tree Models
#Train Tree Model 1
train_tree1 <- rpart(Author ~ ., data = train, method="class", control=rpart.control(cp=0))
summary(train_tree1)
#predict the test dataset using the model for train tree No. 1
predicted1= predict(train_tree1, test, type="class")
#plot number of splits
rsq.rpart(train_tree1)
#plot the decision tree
fancyRpartPlot(train_tree1)
#confusion matrix to find correct and incorrect predictions
table(Authorship=predicted1, true=test$Author)
#Train Tree Model 2
train_tree2 <- rpart(Author ~ ., data = train, method="class", control=rpart.control(cp=0, minsplit = 2, maxdepth = 5))
summary(train_tree2)
#predict the test dataset using the model for train tree No. 1
predicted2= predict(train_tree2, test, type="class")
#plot number of splits
rsq.rpart(train_tree2)
plotcp(train_tree2)
#plot the decision tree
fancyRpartPlot(train_tree2)
#plot the decision tree
fancyRpartPlot(train_tree2)
# Import data
Eurojobs <- read.csv(file = "~/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
SimpleEuroJobs = EuroJobs[,1:2]
SimpleEuroJobs = Eurojobs[,1:2]
View(SimpleEuroJobs)
plot(SimpleEuroJobs)
?plot
SimpleEuroJobs = Eurojobs[,1:3]
View(SimpleEuroJobs)
View(SimpleEuroJobs)
plot(SimpleEuroJobs$Agr, SimpleEuroJobs$Min)
plot(SimpleEuroJobs$Agr, SimpleEuroJobs$Min)
View(SimpleEuroJobs)
SimpleEuroJobs = Eurojobs[,1:2]
plot(SimpleEuroJobs$Agr, SimpleEuroJobs$Min)
model <- kmeans(SimpleEuroJobs, centers = 2)
print(model$cluster)
library(cluster)
library(factoextra)
set.seed(42)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs = Eurojobs[,1:2]
View(Eurojobs)
Eurojobs = Eurojobs[,1:2,1:10]
plot(Eurojobs$Agr, Eurojobs$Min)
Eurojobs = Eurojobs[,1:2,1:10]
Eurojobs = Eurojobs[,1:2,10]
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs = Eurojobs[,1:2,10]
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs = Eurojobs[,1:2,1:10]
View(Eurojobs)
plot(Eurojobs$Agr, Eurojobs$Min)
model <- kmeans(Eurojobs, centers = 2)
print(model$cluster)
set.seed(42)
fviz_cluster(model, Eurojobs, ellipse.type = "norm")
colnames(Eurojobs) <- c("Agriculture", "Mining")
View(Eurojobs)
library(cluster)
library(factoextra)
# Import data
Eurojobs <- read.csv(file = "C:/Users/HQRGRS27/Downloads/Eurojobs.csv", sep = ",", dec = ".", header = TRUE, row.names = 1)
Eurojobs <- Eurojobs[,1:2,]
colnames(Eurojobs) <- c("Agriculture", "Mining")
plot(x = Eurojobs$Agriculture, y = Eurojobs$Mining,
xlab = "Agriculture", ylab="Mining")
xlab = "Agriculture", ylab="Mining", rownames(labels=EuroJobs)
plot(x = Eurojobs$Agriculture, y = Eurojobs$Mining,
xlab = "Agriculture", ylab="Mining", rownames(labels=EuroJobs))
plot(Eurojobs$Mining ~ Eurojobs$Agriculture, xlab = "Agriculture", ylab="Mining")
plot(Eurojobs$Mining ~ Eurojobs$Agriculture, xlab = "Agriculture", ylab="Mining", pch=19)
identify(Eurojobs$Mining ~ Eurojobs$Agriculture, labels = rownames(Eurojobs))
